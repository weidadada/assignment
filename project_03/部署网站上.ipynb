{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "#encoding: utf-8\n",
    "import numpy as np\n",
    "# np.random.seed(42)\n",
    "import pandas as pd\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# # from sklearn.metrics import roc_auc_score\n",
    "# from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "# from keras.models import Model\n",
    "# from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "# from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "# from keras.preprocessing import text, sequence\n",
    "# from keras.callbacks import Callback\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "model = load_model('./first_model.h5')\n",
    "\n",
    "import jieba\n",
    "import re\n",
    "def cut(string):\n",
    "    return ' '.join(jieba.cut(re.sub(r'[^\\w\\s]',' ',string.strip())))\n",
    "\n",
    "\n",
    "\n",
    "EMBEDDING_FILE = './sgns.weibo.word'  # 词向量模型\n",
    "\n",
    "\n",
    "def a():\n",
    "    train = pd.read_csv('./train/sentiment_analysis_trainingset.csv')\n",
    "    test = pd.read_csv('./validation/sentiment_analysis_validationset.csv')\n",
    "    max_features = 30000\n",
    "    print(\"a\")\n",
    "    test['content_cut'] = test[\"content\"].fillna('').apply(cut)\n",
    "    X_test = test[\"content_cut\"].values\n",
    "    print(X_test[1])\n",
    "    print(\"b\")\n",
    "    train['content_cut'] = train[\"content\"].fillna('').apply(cut)\n",
    "    print(\"c\")\n",
    "    X_train = train[\"content_cut\"].values\n",
    "    print(X_train[1])\n",
    "    filters = '！。，·、；：‘’“”？!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    tokenizer = text.Tokenizer(num_words=max_features,\n",
    "                               filters=filters)  # 用来对文本中的词进行统计计数，生成文档词典，以支持基于词典位序生成文本的向量表示。处理的最大单词数量。少于此数的单词丢掉\n",
    "    tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "    return tokenizer\n",
    "\n",
    "def b(str,tokenizer):\n",
    "    maxlen = 100\n",
    "#     embed_size = 300\n",
    "    X_train = tokenizer.texts_to_sequences(str)  # 将多个文档转换为word下标的向量形式,shape为[len(texts)，len(text)] -- (文档数，每条文档的长度)\n",
    "    print(\"d\")\n",
    "    x_train = sequence.pad_sequences(X_train, maxlen=maxlen)  # 将多个序列截断或补齐为相同长度。\n",
    "    return x_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EMBEDDING_FILE, encoding='utf8') as f:\n",
    "    pretrained_embedding = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_idx = dict(get_coefs(*line.strip().split()) for line in pretrained_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 30000\n",
    "embed_size = 300\n",
    "word_index = tokenizer.word_index#将单词（字符串）映射为它们的排名或者索引。\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embedding_idx.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./first_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "  趁着 国庆节   一家人 在 白天 在 山里 玩耍 之后   晚上 决定 吃 李记 搅团   \n",
      " 东门外 这家 店门口 停车 太难 了   根本 没空 位置   所以 停 在 了 旁边 的 地下 停车场   \n",
      " 一 进去 人满为患     应该 是 过节 的 原因 吧   还 等 了 一会 座 我们 一行 8 个人   用 两个 小桌子 拼 的   \n",
      " 我们 点 了   凉拌 西 葫芦丝   菜量 很大   一 大碗   而且 不 贵   菜 上来 之后 服务员 才 浇 上 醋 汁   味道 也 不错   \n",
      "   黑 腐竹 拌 豆王   名字 大概 是 这个   腐竹 是 黑色 的   并 不 像 一般 凉调 菜 的 味道 很 普通   反而 味道 很 惊艳   \n",
      "   油饼   我们 要 了 10 个   一 上来 热热 的 蓬蓬 的   看着 特别 有 食欲   还配 了 小菜   大家 都 说 油饼 特别 好吃   配 小菜 吃 也 很 美味   推荐 推荐   \n",
      "   搅团   没有 要烂 大街 的 浆水 搅团   而是 要 了 水 煮 搅团   其实 和 水 煮 肉片 类似   里面 除了 有 搅 团外 还有 腰花 肥肠 等 还有 一些 蔬菜   搅团 吃 上 真心 不错   面 和 的 刚刚 好   五星   \n",
      " 还有 一个 汤   好像 是 西红柿 鸡蛋 面疙瘩 那种 的   也 特别 家常   量 超大   味道 也 不错   \n",
      " 剩下 就 点 了 两三个 家常菜 啦 这里 就 不 一一 推荐 啦   \n",
      " 我们 8 个人 一共 花 了 200 左右   用 了 代金券   而且 团购 的 时候 原价 85 的 代金券 团购 那天 刚好 73   所以 一共 花 了 150 左右   所以 人均 才 20   吃 的 撑 撑 的   棒   \n",
      " 总体 来说 他家 家常菜 价格 不 贵   好多 素菜 都 在 10 元 以内   味道 也 很 好   很 适合 一家人 来 吃 吃 家常便饭   \n",
      " 祝 生意兴隆 啦   还会 再 光临 哒    \n",
      "b\n",
      "c\n",
      "  第三次 参加 大众 点评 网 霸王餐 的 活动   这家 店 给 人 整体 感觉 一般   首先 环境 只能 算 中等   其次 霸王餐 提供 的 菜品 也 不是 很多   当然 商家 为了 避免 参加 霸王餐 吃不饱 的 现象   给 每桌 都 提供 了 至少 六份 主食   我们 那桌 都 提供 了 两份 年糕   第一次 吃火锅 会 在 桌上 有 这么 多 的 主食 了   整体 来说 这家 火锅店 没有 什么 特别 有 特色 的   不过 每份 菜品 分量 还是 比较 足 的   这点 要 肯定   至于 价格   因为 没有 看 菜单 不 了解   不过 我 看 大众 有 这家 店 的 团购 代金券   相当于 7 折   应该 价位 不会 很 高 的   最后 还是 要 感谢 商家 提供 霸王餐   祝 生意兴隆   财源 广进  \n"
     ]
    }
   ],
   "source": [
    "tokenizer = a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c(s,tokenizer,model):\n",
    "\n",
    "    print(\"tokenizer:{}\".format(tokenizer))\n",
    "    if tokenizer is None:\n",
    "        tokenizer = a()\n",
    "    x_test = b(s,tokenizer)\n",
    "    print(x_test)\n",
    "    y_pred = model.predict(x_test, batch_size=1024)\n",
    "    print(y_pred)\n",
    "    y = y_pred[1].reshape((20,4))\n",
    "    pre = np.argmax(y,axis=1)\n",
    "    return pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"哎，想当年来佘山的时候，啥都没有，三品香算镇上最大看起来最像样的饭店了。菜品多，有点太多，感觉啥都有，杂都不足以形容。随便点些，居然口味什么的都好还可以，价钱自然是便宜当震惊。元宝虾和椒盐九肚鱼都不错吃。不过近来几次么，味道明显没以前好了。冷餐里面一个凉拌海带丝还可以，酸酸甜甜的。镇上也有了些别的大点的饭店，所以不是每次必来了。对了，这家的生意一如既往的超级好，不定位基本吃不到。不过佘山这边的人吃晚饭很早的，所以稍微晚点去就很空了。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = [cut(s)] + [cut(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer:<keras_preprocessing.text.Tokenizer object at 0x0000018A19814B38>\n",
      "d\n",
      "[[    1   571     2    74    31    42   100    31    18   397     7     8\n",
      "  17216  2635   619  7571   371    58    78     1     7    12    14    19\n",
      "    817  1141     3   168   848  5682  9026   114    17  1356 11164     7\n",
      "     15     5    51 11099   725   553    13   693    49   270    12     2\n",
      "  23375    47    32  1146  3700    14    19  1234     1 18264     6     8\n",
      "      2   530   521  7799     1   571    63    53   305 16225     2   155\n",
      "      2    45     1   339  1534     1   196    12    16  2194   281     5\n",
      "    425    51   500     1    25  4261  4914     1    63   415  7453    22\n",
      "      9     4  1697     2]\n",
      " [    1   571     2    74    31    42   100    31    18   397     7     8\n",
      "  17216  2635   619  7571   371    58    78     1     7    12    14    19\n",
      "    817  1141     3   168   848  5682  9026   114    17  1356 11164     7\n",
      "     15     5    51 11099   725   553    13   693    49   270    12     2\n",
      "  23375    47    32  1146  3700    14    19  1234     1 18264     6     8\n",
      "      2   530   521  7799     1   571    63    53   305 16225     2   155\n",
      "      2    45     1   339  1534     1   196    12    16  2194   281     5\n",
      "    425    51   500     1    25  4261  4914     1    63   415  7453    22\n",
      "      9     4  1697     2]]\n",
      "[[4.5148182e-01 1.0966599e-02 3.2590330e-02 4.2817682e-01 9.5406801e-02\n",
      "  4.3794513e-03 8.5811317e-03 8.8267523e-01 1.2876877e-01 2.8047323e-02\n",
      "  2.5306612e-02 8.0079925e-01 2.1763086e-02 2.4622679e-02 7.9904199e-03\n",
      "  9.3243837e-01 1.0270500e-01 3.0771106e-02 1.3207674e-02 8.3534253e-01\n",
      "  1.1750907e-02 6.3630342e-03 6.5883100e-03 9.7519088e-01 1.4518112e-02\n",
      "  7.9213977e-03 2.7063787e-03 9.8042345e-01 1.4905590e-01 3.0948901e-01\n",
      "  1.5020680e-01 1.5514404e-01 3.8822055e-02 9.4362497e-03 3.5869181e-03\n",
      "  9.3901968e-01 4.6609491e-02 8.6705357e-02 3.5007000e-03 8.9615333e-01\n",
      "  9.5702916e-02 5.2561879e-02 8.3024800e-03 8.8093913e-01 5.1521003e-02\n",
      "  1.8250525e-02 5.2254200e-03 9.1711587e-01 8.3151460e-02 8.3366036e-02\n",
      "  2.7737886e-02 7.5605536e-01 6.6415071e-02 1.4226526e-02 1.0880172e-02\n",
      "  9.0124476e-01 8.0109864e-02 4.4903696e-02 3.5307586e-02 8.5486484e-01\n",
      "  6.6462582e-01 1.7902559e-01 4.4091642e-03 1.8318653e-02 7.0343703e-02\n",
      "  1.8807232e-02 8.2803667e-03 8.7611330e-01 1.8466026e-02 5.8950782e-03\n",
      "  2.8771162e-03 9.6995258e-01 6.1479396e-01 1.6809601e-01 1.0237962e-02\n",
      "  2.0740449e-02 2.7536958e-02 4.9791038e-03 7.3146820e-04 9.5921969e-01]\n",
      " [4.5148182e-01 1.0966599e-02 3.2590330e-02 4.2817682e-01 9.5406801e-02\n",
      "  4.3794513e-03 8.5811317e-03 8.8267523e-01 1.2876877e-01 2.8047323e-02\n",
      "  2.5306612e-02 8.0079925e-01 2.1763086e-02 2.4622679e-02 7.9904199e-03\n",
      "  9.3243837e-01 1.0270500e-01 3.0771106e-02 1.3207674e-02 8.3534253e-01\n",
      "  1.1750907e-02 6.3630342e-03 6.5883100e-03 9.7519088e-01 1.4518112e-02\n",
      "  7.9213977e-03 2.7063787e-03 9.8042345e-01 1.4905590e-01 3.0948901e-01\n",
      "  1.5020680e-01 1.5514404e-01 3.8822055e-02 9.4362497e-03 3.5869181e-03\n",
      "  9.3901968e-01 4.6609491e-02 8.6705357e-02 3.5007000e-03 8.9615333e-01\n",
      "  9.5702916e-02 5.2561879e-02 8.3024800e-03 8.8093913e-01 5.1521003e-02\n",
      "  1.8250525e-02 5.2254200e-03 9.1711587e-01 8.3151460e-02 8.3366036e-02\n",
      "  2.7737886e-02 7.5605536e-01 6.6415071e-02 1.4226526e-02 1.0880172e-02\n",
      "  9.0124476e-01 8.0109864e-02 4.4903696e-02 3.5307586e-02 8.5486484e-01\n",
      "  6.6462582e-01 1.7902559e-01 4.4091642e-03 1.8318653e-02 7.0343703e-02\n",
      "  1.8807232e-02 8.2803667e-03 8.7611330e-01 1.8466026e-02 5.8950782e-03\n",
      "  2.8771162e-03 9.6995258e-01 6.1479396e-01 1.6809601e-01 1.0237962e-02\n",
      "  2.0740449e-02 2.7536958e-02 4.9791038e-03 7.3146820e-04 9.5921969e-01]]\n"
     ]
    }
   ],
   "source": [
    "pre = c(s1,tokenizer,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 0, 3],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-2 1 -2 -2 -2 -2 -2 0 -2 0 -2 -2 -2 -2 1 1 -2 -2 1 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
