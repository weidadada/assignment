{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    Each node in neural networks will have these attributes and methods\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs=[]):\n",
    "        \"\"\"\n",
    "        if the node is the operator of \"ax + b\", the inputs will be x node , and the outputs \n",
    "        of this is its successors. \n",
    "        \n",
    "        and the value is *ax + b*\n",
    "        \"\"\"\n",
    "        self.inputs = inputs # input_list <- C, Java <- 匈牙利命名法 -> Python 特别不建议\n",
    "       # self.outputs = outputs # output_list \n",
    "        self.value = None \n",
    "        self.outputs = []\n",
    "        self.gradients = {}\n",
    "        \n",
    "        for node in self.inputs:#inputs2个点，list[A,B],A的输出加上B的输出\n",
    "            node.outputs.append(self) # build a connection relationship\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"Forward propogation\n",
    "        \n",
    "        compute the output value based on input nodes and store the value \n",
    "        into *self.value*\n",
    "        \"\"\"\n",
    "        raise NotImplemented #虚类，要求其子类一定要实现，不实现的时候会导致问题\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\" Back propogation\n",
    "        \n",
    "        compute the gradient of each input node and store the value \n",
    "        into \"self.gredients\"\n",
    "        \"\"\"\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Input(Node):\n",
    "    def __init__(self, name=''):\n",
    "        Node.__init__(self, inputs=[])\n",
    "        self.name= name\n",
    "    \n",
    "    def forward(self, value=None):\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "        \n",
    "    def backward(self):\n",
    "        self.gradients = {}\n",
    "        \n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self] = grad_cost\n",
    "    \n",
    "#     def __repr__(self):\n",
    "#         return 'Input Node: {}'.format(self.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Node):\n",
    "    def __init__(self, nodes, weights, bias):\n",
    "        self.w_node = weights\n",
    "        self.x_node = nodes\n",
    "        self.b_node = bias\n",
    "        Node.__init__(self, inputs=[nodes, weights, bias])\n",
    "    \n",
    "    def forward(self): \n",
    "        \"\"\"compute the wx + b using numpy\"\"\"\n",
    "        self.value = np.dot(self.x_node.value, self.w_node.value) + self.b_node.value#np.dot(X, W) + B\n",
    "        \n",
    "    \n",
    "    def backward(self):\n",
    "        \n",
    "        for node in self.outputs:\n",
    "            #gradient_of_loss_of_this_output_node = node.gradient[self]\n",
    "            grad_cost = node.gradients[self]\n",
    "            \n",
    "            self.gradients[self.w_node] = np.dot(self.x_node.value.T, grad_cost)\n",
    "            self.gradients[self.b_node] = np.sum(grad_cost * 1, axis=0, keepdims=False)\n",
    "            self.gradients[self.x_node] = np.dot(grad_cost, self.w_node.value.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Node):\n",
    "    def __init__(self, node):\n",
    "        Node.__init__(self, [node])\n",
    "        self.x_node = node\n",
    "    \n",
    "    def _sigmoid(self, x):\n",
    "        return 1. / (1 + np.exp(-1 * x))\n",
    "    \n",
    "    def forward(self):\n",
    "        self.value = self._sigmoid(self.x_node.value)\n",
    "    \n",
    "    def backward(self):\n",
    "        y = self.value\n",
    "        \n",
    "        self.partial = y * (1 - y)\n",
    "        \n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]\n",
    "            \n",
    "            self.gradients[self.x_node] = grad_cost * self.partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Node):#loss\n",
    "    def __init__(self, y_true, y_hat):\n",
    "        self.y_true_node = y_true\n",
    "        self.y_hat_node = y_hat\n",
    "        Node.__init__(self, inputs=[y_true, y_hat])\n",
    "    \n",
    "    def forward(self):\n",
    "        y_true_flatten = self.y_true_node.value.reshape(-1, 1)\n",
    "        y_hat_flatten = self.y_hat_node.value.reshape(-1, 1)\n",
    "        \n",
    "        self.diff = y_true_flatten - y_hat_flatten\n",
    "        \n",
    "        self.value = np.mean(self.diff**2)\n",
    "        \n",
    "    def backward(self):\n",
    "        n = self.y_hat_node.value.shape[0]\n",
    "        \n",
    "        self.gradients[self.y_true_node] = (2 / n) * self.diff\n",
    "        self.gradients[self.y_hat_node] =  (-2 / n) * self.diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_one_batch(topological_sorted_graph):\n",
    "    # graph 是经过拓扑排序之后的 一个list\n",
    "    for node in topological_sorted_graph:\n",
    "        node.forward()\n",
    "        \n",
    "    for node in topological_sorted_graph[::-1]:\n",
    "        node.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topological_sort(data_with_value):\n",
    "    feed_dict = data_with_value \n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outputs:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "            ## if n is Input Node, set n'value as \n",
    "            ## feed_dict[n]\n",
    "            ## else, n's value is caculate as its\n",
    "            ## inbounds\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outputs:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_update(trainable_nodes, learning_rate=1e-2):\n",
    "    for t in trainable_nodes:\n",
    "        t.value += -1 * learning_rate * t.gradients[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "data = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = data['data']\n",
    "y_ = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)#axis=0列，np.std标准差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X_.shape[1]#特征数\n",
    "n_hidden = 10\n",
    "# n_hidden_2 = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1_, b1_ = np.random.randn(n_features, n_hidden), np.zeros(n_hidden)#标准正态分布\n",
    "W2_, b2_ = np.random.randn(n_hidden, 1), np.zeros(1)\n",
    "# W2_, b2_ = np.random.randn(n_hidden, n_hidden_2), np.zeros(n_hidden_2)\n",
    "# W3_, b3_ = np.random.randn(n_hidden_2, 1), np.zeros(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.28510275],\n",
       "       [ 0.3849249 ],\n",
       "       [-0.2894206 ],\n",
       "       [ 1.00699906],\n",
       "       [ 0.12492095],\n",
       "       [ 0.73470094],\n",
       "       [ 0.26775248],\n",
       "       [ 0.17709125],\n",
       "       [-0.44695528],\n",
       "       [-1.33587122]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = Input(name='X'), Input(name='y')  # tensorflow -> placeholder\n",
    "W1, b1 = Input(name='W1'), Input(name='b1')\n",
    "W2, b2 = Input(name='W2'), Input(name='b2')\n",
    "# W3, b3 = Input(name='W3'), Input(name='b3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_output = Linear(X, W1, b1)\n",
    "sigmoid_output = Sigmoid(linear_output)\n",
    "# linear_output_2 = Linear(sigmoid_output, W2, b2)\n",
    "# sigmoid_output_2 = Sigmoid(linear_output_2)\n",
    "# yhat = Linear(sigmoid_output_2, W3, b3)\n",
    "yhat = Linear(sigmoid_output, W2, b2)\n",
    "loss = MSE(y, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_node_with_value = {  # -> feed_dict \n",
    "    X: X_, \n",
    "    y: y_, \n",
    "    W1: W1_, \n",
    "    W2: W2_,\n",
    "#     W3: W3_, \n",
    "    b1: b1_, \n",
    "    b2: b2_,\n",
    "#     b3: b3_ \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = topological_sort(input_node_with_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Input at 0x1bde89256a0>,\n",
       " <__main__.Input at 0x1bde89256d8>,\n",
       " <__main__.Input at 0x1bde8925710>,\n",
       " <__main__.Input at 0x1bde8925748>,\n",
       " <__main__.Input at 0x1bde8925780>,\n",
       " <__main__.Input at 0x1bde89257f0>,\n",
       " <__main__.Linear at 0x1bde8917c88>,\n",
       " <__main__.Sigmoid at 0x1bde8917d30>,\n",
       " <__main__.Linear at 0x1bde8917518>,\n",
       " <__main__.MSE at 0x1bde8917e10>]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.choice(range(100), size=10, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run(dictionary):\n",
    "#     return topological_sort(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss = 568.935\n",
      "Epoch: 101, loss = 29.804\n",
      "Epoch: 201, loss = 24.299\n",
      "Epoch: 301, loss = 20.325\n",
      "Epoch: 401, loss = 13.945\n",
      "Epoch: 501, loss = 16.803\n",
      "Epoch: 601, loss = 16.946\n",
      "Epoch: 701, loss = 12.748\n",
      "Epoch: 801, loss = 11.247\n",
      "Epoch: 901, loss = 10.763\n",
      "Epoch: 1001, loss = 13.298\n",
      "Epoch: 1101, loss = 8.208\n",
      "Epoch: 1201, loss = 9.997\n",
      "Epoch: 1301, loss = 8.152\n",
      "Epoch: 1401, loss = 11.153\n",
      "Epoch: 1501, loss = 8.960\n",
      "Epoch: 1601, loss = 8.964\n",
      "Epoch: 1701, loss = 8.775\n",
      "Epoch: 1801, loss = 10.090\n",
      "Epoch: 1901, loss = 7.059\n",
      "Epoch: 2001, loss = 8.255\n",
      "Epoch: 2101, loss = 7.677\n",
      "Epoch: 2201, loss = 7.822\n",
      "Epoch: 2301, loss = 7.896\n",
      "Epoch: 2401, loss = 7.610\n",
      "Epoch: 2501, loss = 8.279\n",
      "Epoch: 2601, loss = 8.474\n",
      "Epoch: 2701, loss = 8.425\n",
      "Epoch: 2801, loss = 8.306\n",
      "Epoch: 2901, loss = 6.564\n",
      "Epoch: 3001, loss = 6.636\n",
      "Epoch: 3101, loss = 7.565\n",
      "Epoch: 3201, loss = 5.592\n",
      "Epoch: 3301, loss = 6.556\n",
      "Epoch: 3401, loss = 7.951\n",
      "Epoch: 3501, loss = 9.145\n",
      "Epoch: 3601, loss = 6.019\n",
      "Epoch: 3701, loss = 6.614\n",
      "Epoch: 3801, loss = 5.509\n",
      "Epoch: 3901, loss = 6.372\n",
      "Epoch: 4001, loss = 5.835\n",
      "Epoch: 4101, loss = 6.498\n",
      "Epoch: 4201, loss = 5.784\n",
      "Epoch: 4301, loss = 5.598\n",
      "Epoch: 4401, loss = 5.069\n",
      "Epoch: 4501, loss = 5.843\n",
      "Epoch: 4601, loss = 6.250\n",
      "Epoch: 4701, loss = 5.252\n",
      "Epoch: 4801, loss = 7.131\n",
      "Epoch: 4901, loss = 6.361\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "epochs = 5000\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "steps_per_epoch = X_.shape[0] // batch_size\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    \n",
    "    for batch in range(steps_per_epoch):\n",
    "        #indices = np.random.choice(range(X_.shape[0]), size=10, replace=True)\n",
    "        #X_batch = X_[indices]\n",
    "        #y_batch = y_[indices]\n",
    "        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n",
    "        \n",
    "        X.value = X_batch\n",
    "        y.value = y_batch\n",
    "        \n",
    "#         input_node_with_value = {  # -> feed_dict \n",
    "#             X: X_batch, \n",
    "#             y: y_batch, \n",
    "#             W1: W1.value, \n",
    "#             W2: W2.value, \n",
    "#             b1: b1.value, \n",
    "#             b2: b2.value,\n",
    "#         }\n",
    "        \n",
    "#         graph = topological_sort(input_node_with_value)\n",
    "        \n",
    "        training_one_batch(graph)\n",
    "        \n",
    "        learning_rate = 1e-3\n",
    "        \n",
    "#         sgd_update(trainable_nodes=[W1, W2, b1, b2,W3,b3], learning_rate=learning_rate)\n",
    "        sgd_update(trainable_nodes=[W1, W2, b1, b2], learning_rate=learning_rate)\n",
    "        \n",
    "        loss += graph[-1].value\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        print('Epoch: {}, loss = {:.3f}'.format(i+1, loss/steps_per_epoch))\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1bde957e2b0>]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# “⾖豆瓣评论” classiﬁcation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3049: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京意淫到了脑残的地步，看了恶心想吐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>中二得很</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>“犯我中华者，虽远必诛”，吴京比这句话还要意淫一百倍。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>脑子是个好东西，希望编剧们都能有。</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>三星半，实打实的7分。第一集在爱国主旋律内部做着各种置换与较劲，但第二集才真正显露吴京的野心...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>开篇长镜头惊险大气引人入胜 结合了水平不俗的快剪下实打实的真刀真枪 让人不禁热血沸腾 特别弹...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>15/100吴京的冷峰在这部里即像成龙，又像杰森斯坦森，但体制外的同类型电影，主角总是代表个...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                        link name  \\\n",
       "0   1  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "1   2  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "2   3  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "3   4  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "4   5  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "5   6  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "6   7  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "7   8  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "8   9  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "9  10  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "\n",
       "                                             comment star  \n",
       "0                                 吴京意淫到了脑残的地步，看了恶心想吐    1  \n",
       "1  首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...    2  \n",
       "2  吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...    2  \n",
       "3                      凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。    4  \n",
       "4                                               中二得很    1  \n",
       "5                        “犯我中华者，虽远必诛”，吴京比这句话还要意淫一百倍。    1  \n",
       "6                                  脑子是个好东西，希望编剧们都能有。    2  \n",
       "7  三星半，实打实的7分。第一集在爱国主旋律内部做着各种置换与较劲，但第二集才真正显露吴京的野心...    4  \n",
       "8  开篇长镜头惊险大气引人入胜 结合了水平不俗的快剪下实打实的真刀真枪 让人不禁热血沸腾 特别弹...    4  \n",
       "9  15/100吴京的冷峰在这部里即像成龙，又像杰森斯坦森，但体制外的同类型电影，主角总是代表个...    1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "content = pd.read_csv('./movie_comments.csv')#读取文件\n",
    "content.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content_null = content.isnull().sum()#产看缺失值\n",
    "# content = content.dropna()#除去缺失值\n",
    "# content_all = content.reset_index(drop=True)#重建索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261497"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (content_all.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re \n",
    "def cut(string):\n",
    "    return ' '.join(jieba.cut(re.sub(r'[^\\w\\s]',' ',string.strip())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\MSI\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.365 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "content['content_cut'] = content[\"comment\"].fillna('').apply(cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "      <th>content_cut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京意淫到了脑残的地步，看了恶心想吐</td>\n",
       "      <td>1</td>\n",
       "      <td>吴京 意淫 到 了 脑残 的 地步   看 了 恶心 想 吐</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...</td>\n",
       "      <td>2</td>\n",
       "      <td>首映礼 看 的   太 恐怖 了 这个 电影   不讲道理 的   完全 就是 吴京 在 实...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...</td>\n",
       "      <td>2</td>\n",
       "      <td>吴京 的 炒作 水平 不输 冯小刚   但小刚 至少 不会 用 主旋律 来 炒作   吴京 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。</td>\n",
       "      <td>4</td>\n",
       "      <td>凭良心说   好 看到 不像   战狼 1   的 续集   完虐   湄公河 行动</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>中二得很</td>\n",
       "      <td>1</td>\n",
       "      <td>中二得 很</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>“犯我中华者，虽远必诛”，吴京比这句话还要意淫一百倍。</td>\n",
       "      <td>1</td>\n",
       "      <td>犯 我 中华 者   虽远必 诛     吴京 比 这句 话 还要 意淫 一百倍</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>脑子是个好东西，希望编剧们都能有。</td>\n",
       "      <td>2</td>\n",
       "      <td>脑子 是 个 好 东西   希望 编剧 们 都 能 有</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>三星半，实打实的7分。第一集在爱国主旋律内部做着各种置换与较劲，但第二集才真正显露吴京的野心...</td>\n",
       "      <td>4</td>\n",
       "      <td>三星 半   实打实 的 7 分   第一集 在 爱国 主旋律 内部 做 着 各种 置换 与...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>开篇长镜头惊险大气引人入胜 结合了水平不俗的快剪下实打实的真刀真枪 让人不禁热血沸腾 特别弹...</td>\n",
       "      <td>4</td>\n",
       "      <td>开篇 长镜头 惊险 大气 引人入胜   结合 了 水平 不俗 的 快 剪下 实打实 的 真刀...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>15/100吴京的冷峰在这部里即像成龙，又像杰森斯坦森，但体制外的同类型电影，主角总是代表个...</td>\n",
       "      <td>1</td>\n",
       "      <td>15   100 吴京 的 冷峰 在 这部 里 即 像 成龙   又 像杰 森斯坦 森   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                        link name  \\\n",
       "0   1  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "1   2  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "2   3  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "3   4  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "4   5  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "5   6  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "6   7  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "7   8  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "8   9  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "9  10  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "\n",
       "                                             comment star  \\\n",
       "0                                 吴京意淫到了脑残的地步，看了恶心想吐    1   \n",
       "1  首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...    2   \n",
       "2  吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...    2   \n",
       "3                      凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。    4   \n",
       "4                                               中二得很    1   \n",
       "5                        “犯我中华者，虽远必诛”，吴京比这句话还要意淫一百倍。    1   \n",
       "6                                  脑子是个好东西，希望编剧们都能有。    2   \n",
       "7  三星半，实打实的7分。第一集在爱国主旋律内部做着各种置换与较劲，但第二集才真正显露吴京的野心...    4   \n",
       "8  开篇长镜头惊险大气引人入胜 结合了水平不俗的快剪下实打实的真刀真枪 让人不禁热血沸腾 特别弹...    4   \n",
       "9  15/100吴京的冷峰在这部里即像成龙，又像杰森斯坦森，但体制外的同类型电影，主角总是代表个...    1   \n",
       "\n",
       "                                         content_cut  \n",
       "0                     吴京 意淫 到 了 脑残 的 地步   看 了 恶心 想 吐  \n",
       "1  首映礼 看 的   太 恐怖 了 这个 电影   不讲道理 的   完全 就是 吴京 在 实...  \n",
       "2  吴京 的 炒作 水平 不输 冯小刚   但小刚 至少 不会 用 主旋律 来 炒作   吴京 ...  \n",
       "3     凭良心说   好 看到 不像   战狼 1   的 续集   完虐   湄公河 行动      \n",
       "4                                              中二得 很  \n",
       "5         犯 我 中华 者   虽远必 诛     吴京 比 这句 话 还要 意淫 一百倍    \n",
       "6                      脑子 是 个 好 东西   希望 编剧 们 都 能 有    \n",
       "7  三星 半   实打实 的 7 分   第一集 在 爱国 主旋律 内部 做 着 各种 置换 与...  \n",
       "8  开篇 长镜头 惊险 大气 引人入胜   结合 了 水平 不俗 的 快 剪下 实打实 的 真刀...  \n",
       "9  15   100 吴京 的 冷峰 在 这部 里 即 像 成龙   又 像杰 森斯坦 森   ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "      <th>content_cut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>33</td>\n",
       "      <td>https://movie.douban.com/subject/2567647/</td>\n",
       "      <td>福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q</td>\n",
       "      <td>2013.04.25 掉渣天的画质</td>\n",
       "      <td>5</td>\n",
       "      <td>2013   04   25   掉 渣 天 的 画质</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>34</td>\n",
       "      <td>https://movie.douban.com/subject/2567647/</td>\n",
       "      <td>福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q</td>\n",
       "      <td>只有凌波，我一定要救出来！新加：上映当天就被剧透到死了，今天枪版观看完毕，说不上来的感觉，是...</td>\n",
       "      <td>5</td>\n",
       "      <td>只有 凌波   我 一定 要 救 出来   新加   上映 当天 就 被 剧透到 死 了  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>35</td>\n",
       "      <td>https://movie.douban.com/subject/2567647/</td>\n",
       "      <td>福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q</td>\n",
       "      <td>距离发动失败的第三次冲击已经14年的时间，整个世界已经不一样，碇真嗣再一次被导向发动第四次冲...</td>\n",
       "      <td>4</td>\n",
       "      <td>距离 发动 失败 的 第三次 冲击 已经 14 年 的 时间   整个 世界 已经 不 一样...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>36</td>\n",
       "      <td>https://movie.douban.com/subject/2567647/</td>\n",
       "      <td>福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q</td>\n",
       "      <td>这到底神马情况，各种乱入..........</td>\n",
       "      <td>3</td>\n",
       "      <td>这 到底 神马 情况   各种 乱入</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>37</td>\n",
       "      <td>https://movie.douban.com/subject/2567647/</td>\n",
       "      <td>福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q</td>\n",
       "      <td>多么中二的剧情，就是一群小青年想玩文艺，来着全人类做铺垫的故事，三星完全是给作画的，剧情就是一坨屎</td>\n",
       "      <td>3</td>\n",
       "      <td>多么 中二 的 剧情   就是 一群 小青年 想 玩 文艺   来 着 全人类 做 铺垫 的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>38</td>\n",
       "      <td>https://movie.douban.com/subject/2567647/</td>\n",
       "      <td>福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q</td>\n",
       "      <td>疯了</td>\n",
       "      <td>5</td>\n",
       "      <td>疯 了</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>39</td>\n",
       "      <td>https://movie.douban.com/subject/2567647/</td>\n",
       "      <td>福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q</td>\n",
       "      <td>gay死它得了。碇真治对㶆薰那一脸的菊花湿润瘙痒啊。。。</td>\n",
       "      <td>3</td>\n",
       "      <td>gay 死 它 得 了   碇真治 对 㶆 薰 那 一脸 的 菊花 湿润 瘙痒 啊</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>40</td>\n",
       "      <td>https://movie.douban.com/subject/2567647/</td>\n",
       "      <td>福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q</td>\n",
       "      <td>等我明白了再决定是不是加一星= =</td>\n",
       "      <td>4</td>\n",
       "      <td>等 我 明白 了 再 决定 是不是 加一星</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>id</td>\n",
       "      <td>link</td>\n",
       "      <td>name</td>\n",
       "      <td>comment</td>\n",
       "      <td>star</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>1</td>\n",
       "      <td>https://movie.douban.com/subject/2567647/</td>\n",
       "      <td>福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q</td>\n",
       "      <td>盼了两年，火急火燎全神贯注地看完后……吭爹啊！除了基佬薰必须成一坨血的结局外，这货跟老版TV...</td>\n",
       "      <td>5</td>\n",
       "      <td>盼 了 两年   火急火燎 全神贯注 地看 完后     吭爹 啊   除了 基佬 薰 必须...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                       link                       name  \\\n",
       "560  33  https://movie.douban.com/subject/2567647/  福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q   \n",
       "561  34  https://movie.douban.com/subject/2567647/  福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q   \n",
       "562  35  https://movie.douban.com/subject/2567647/  福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q   \n",
       "563  36  https://movie.douban.com/subject/2567647/  福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q   \n",
       "564  37  https://movie.douban.com/subject/2567647/  福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q   \n",
       "565  38  https://movie.douban.com/subject/2567647/  福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q   \n",
       "566  39  https://movie.douban.com/subject/2567647/  福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q   \n",
       "567  40  https://movie.douban.com/subject/2567647/  福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q   \n",
       "568  id                                       link                       name   \n",
       "569   1  https://movie.douban.com/subject/2567647/  福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q   \n",
       "\n",
       "                                               comment  star  \\\n",
       "560                                  2013.04.25 掉渣天的画质     5   \n",
       "561  只有凌波，我一定要救出来！新加：上映当天就被剧透到死了，今天枪版观看完毕，说不上来的感觉，是...     5   \n",
       "562  距离发动失败的第三次冲击已经14年的时间，整个世界已经不一样，碇真嗣再一次被导向发动第四次冲...     4   \n",
       "563                             这到底神马情况，各种乱入..........     3   \n",
       "564  多么中二的剧情，就是一群小青年想玩文艺，来着全人类做铺垫的故事，三星完全是给作画的，剧情就是一坨屎     3   \n",
       "565                                                 疯了     5   \n",
       "566                       gay死它得了。碇真治对㶆薰那一脸的菊花湿润瘙痒啊。。。     3   \n",
       "567                                  等我明白了再决定是不是加一星= =     4   \n",
       "568                                            comment  star   \n",
       "569  盼了两年，火急火燎全神贯注地看完后……吭爹啊！除了基佬薰必须成一坨血的结局外，这货跟老版TV...     5   \n",
       "\n",
       "                                           content_cut  \n",
       "560                        2013   04   25   掉 渣 天 的 画质  \n",
       "561  只有 凌波   我 一定 要 救 出来   新加   上映 当天 就 被 剧透到 死 了  ...  \n",
       "562  距离 发动 失败 的 第三次 冲击 已经 14 年 的 时间   整个 世界 已经 不 一样...  \n",
       "563             这 到底 神马 情况   各种 乱入                      \n",
       "564  多么 中二 的 剧情   就是 一群 小青年 想 玩 文艺   来 着 全人类 做 铺垫 的...  \n",
       "565                                                疯 了  \n",
       "566    gay 死 它 得 了   碇真治 对 㶆 薰 那 一脸 的 菊花 湿润 瘙痒 啊        \n",
       "567                        等 我 明白 了 再 决定 是不是 加一星        \n",
       "568                                            comment  \n",
       "569  盼 了 两年   火急火燎 全神贯注 地看 完后     吭爹 啊   除了 基佬 薰 必须...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.iloc[560:570]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content = content[~content['star'].isin([\"star\"])]#~取反"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "      <th>content_cut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>33</td>\n",
       "      <td>https://movie.douban.com/subject/2567647/</td>\n",
       "      <td>福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q</td>\n",
       "      <td>2013.04.25 掉渣天的画质</td>\n",
       "      <td>5</td>\n",
       "      <td>2013   04   25   掉 渣 天 的 画质</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>34</td>\n",
       "      <td>https://movie.douban.com/subject/2567647/</td>\n",
       "      <td>福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q</td>\n",
       "      <td>只有凌波，我一定要救出来！新加：上映当天就被剧透到死了，今天枪版观看完毕，说不上来的感觉，是...</td>\n",
       "      <td>5</td>\n",
       "      <td>只有 凌波   我 一定 要 救 出来   新加   上映 当天 就 被 剧透到 死 了  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>35</td>\n",
       "      <td>https://movie.douban.com/subject/2567647/</td>\n",
       "      <td>福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q</td>\n",
       "      <td>距离发动失败的第三次冲击已经14年的时间，整个世界已经不一样，碇真嗣再一次被导向发动第四次冲...</td>\n",
       "      <td>4</td>\n",
       "      <td>距离 发动 失败 的 第三次 冲击 已经 14 年 的 时间   整个 世界 已经 不 一样...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>36</td>\n",
       "      <td>https://movie.douban.com/subject/2567647/</td>\n",
       "      <td>福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q</td>\n",
       "      <td>这到底神马情况，各种乱入..........</td>\n",
       "      <td>3</td>\n",
       "      <td>这 到底 神马 情况   各种 乱入</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>37</td>\n",
       "      <td>https://movie.douban.com/subject/2567647/</td>\n",
       "      <td>福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q</td>\n",
       "      <td>多么中二的剧情，就是一群小青年想玩文艺，来着全人类做铺垫的故事，三星完全是给作画的，剧情就是一坨屎</td>\n",
       "      <td>3</td>\n",
       "      <td>多么 中二 的 剧情   就是 一群 小青年 想 玩 文艺   来 着 全人类 做 铺垫 的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>38</td>\n",
       "      <td>https://movie.douban.com/subject/2567647/</td>\n",
       "      <td>福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q</td>\n",
       "      <td>疯了</td>\n",
       "      <td>5</td>\n",
       "      <td>疯 了</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>39</td>\n",
       "      <td>https://movie.douban.com/subject/2567647/</td>\n",
       "      <td>福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q</td>\n",
       "      <td>gay死它得了。碇真治对㶆薰那一脸的菊花湿润瘙痒啊。。。</td>\n",
       "      <td>3</td>\n",
       "      <td>gay 死 它 得 了   碇真治 对 㶆 薰 那 一脸 的 菊花 湿润 瘙痒 啊</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>40</td>\n",
       "      <td>https://movie.douban.com/subject/2567647/</td>\n",
       "      <td>福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q</td>\n",
       "      <td>等我明白了再决定是不是加一星= =</td>\n",
       "      <td>4</td>\n",
       "      <td>等 我 明白 了 再 决定 是不是 加一星</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>1</td>\n",
       "      <td>https://movie.douban.com/subject/2567647/</td>\n",
       "      <td>福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q</td>\n",
       "      <td>盼了两年，火急火燎全神贯注地看完后……吭爹啊！除了基佬薰必须成一坨血的结局外，这货跟老版TV...</td>\n",
       "      <td>5</td>\n",
       "      <td>盼 了 两年   火急火燎 全神贯注 地看 完后     吭爹 啊   除了 基佬 薰 必须...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>2</td>\n",
       "      <td>https://movie.douban.com/subject/2567647/</td>\n",
       "      <td>福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q</td>\n",
       "      <td>时隔那么多年，明日香又登女神榜，薰继续是大爱！可惜，我依旧讨厌圣母小白什么的！</td>\n",
       "      <td>4</td>\n",
       "      <td>时隔 那么 多年   明日香 又 登 女神 榜   薰 继续 是 大爱   可惜   我 依...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                       link                       name  \\\n",
       "560  33  https://movie.douban.com/subject/2567647/  福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q   \n",
       "561  34  https://movie.douban.com/subject/2567647/  福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q   \n",
       "562  35  https://movie.douban.com/subject/2567647/  福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q   \n",
       "563  36  https://movie.douban.com/subject/2567647/  福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q   \n",
       "564  37  https://movie.douban.com/subject/2567647/  福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q   \n",
       "565  38  https://movie.douban.com/subject/2567647/  福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q   \n",
       "566  39  https://movie.douban.com/subject/2567647/  福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q   \n",
       "567  40  https://movie.douban.com/subject/2567647/  福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q   \n",
       "569   1  https://movie.douban.com/subject/2567647/  福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q   \n",
       "570   2  https://movie.douban.com/subject/2567647/  福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q   \n",
       "\n",
       "                                               comment star  \\\n",
       "560                                  2013.04.25 掉渣天的画质    5   \n",
       "561  只有凌波，我一定要救出来！新加：上映当天就被剧透到死了，今天枪版观看完毕，说不上来的感觉，是...    5   \n",
       "562  距离发动失败的第三次冲击已经14年的时间，整个世界已经不一样，碇真嗣再一次被导向发动第四次冲...    4   \n",
       "563                             这到底神马情况，各种乱入..........    3   \n",
       "564  多么中二的剧情，就是一群小青年想玩文艺，来着全人类做铺垫的故事，三星完全是给作画的，剧情就是一坨屎    3   \n",
       "565                                                 疯了    5   \n",
       "566                       gay死它得了。碇真治对㶆薰那一脸的菊花湿润瘙痒啊。。。    3   \n",
       "567                                  等我明白了再决定是不是加一星= =    4   \n",
       "569  盼了两年，火急火燎全神贯注地看完后……吭爹啊！除了基佬薰必须成一坨血的结局外，这货跟老版TV...    5   \n",
       "570            时隔那么多年，明日香又登女神榜，薰继续是大爱！可惜，我依旧讨厌圣母小白什么的！    4   \n",
       "\n",
       "                                           content_cut  \n",
       "560                        2013   04   25   掉 渣 天 的 画质  \n",
       "561  只有 凌波   我 一定 要 救 出来   新加   上映 当天 就 被 剧透到 死 了  ...  \n",
       "562  距离 发动 失败 的 第三次 冲击 已经 14 年 的 时间   整个 世界 已经 不 一样...  \n",
       "563             这 到底 神马 情况   各种 乱入                      \n",
       "564  多么 中二 的 剧情   就是 一群 小青年 想 玩 文艺   来 着 全人类 做 铺垫 的...  \n",
       "565                                                疯 了  \n",
       "566    gay 死 它 得 了   碇真治 对 㶆 薰 那 一脸 的 菊花 湿润 瘙痒 啊        \n",
       "567                        等 我 明白 了 再 决定 是不是 加一星        \n",
       "569  盼 了 两年   火急火燎 全神贯注 地看 完后     吭爹 啊   除了 基佬 薰 必须...  \n",
       "570  时隔 那么 多年   明日香 又 登 女神 榜   薰 继续 是 大爱   可惜   我 依...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.iloc[560:570]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                             1\n",
       "link                   https://movie.douban.com/subject/2567647/\n",
       "name                                   福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q\n",
       "comment        盼了两年，火急火燎全神贯注地看完后……吭爹啊！除了基佬薰必须成一坨血的结局外，这货跟老版TV...\n",
       "star                                                           5\n",
       "content_cut    盼 了 两年   火急火燎 全神贯注 地看 完后     吭爹 啊   除了 基佬 薰 必须...\n",
       "Name: 569, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.iloc[568]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261496"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols=[x for i,x,in enumerate(content.columns) if content.iat[4,i]== \"null\"]#content.columns列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content_null = content.isnull().sum()#产看缺失值\n",
    "content_all = content.dropna()#除去缺失值\n",
    "content_all = content_all.reset_index(drop=True)#重建索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "      <th>content_cut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>33</td>\n",
       "      <td>https://movie.douban.com/subject/2567647/</td>\n",
       "      <td>福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q</td>\n",
       "      <td>2013.04.25 掉渣天的画质</td>\n",
       "      <td>5</td>\n",
       "      <td>2013   04   25   掉 渣 天 的 画质</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>34</td>\n",
       "      <td>https://movie.douban.com/subject/2567647/</td>\n",
       "      <td>福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q</td>\n",
       "      <td>只有凌波，我一定要救出来！新加：上映当天就被剧透到死了，今天枪版观看完毕，说不上来的感觉，是...</td>\n",
       "      <td>5</td>\n",
       "      <td>只有 凌波   我 一定 要 救 出来   新加   上映 当天 就 被 剧透到 死 了  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>35</td>\n",
       "      <td>https://movie.douban.com/subject/2567647/</td>\n",
       "      <td>福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q</td>\n",
       "      <td>距离发动失败的第三次冲击已经14年的时间，整个世界已经不一样，碇真嗣再一次被导向发动第四次冲...</td>\n",
       "      <td>4</td>\n",
       "      <td>距离 发动 失败 的 第三次 冲击 已经 14 年 的 时间   整个 世界 已经 不 一样...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>36</td>\n",
       "      <td>https://movie.douban.com/subject/2567647/</td>\n",
       "      <td>福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q</td>\n",
       "      <td>这到底神马情况，各种乱入..........</td>\n",
       "      <td>3</td>\n",
       "      <td>这 到底 神马 情况   各种 乱入</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>37</td>\n",
       "      <td>https://movie.douban.com/subject/2567647/</td>\n",
       "      <td>福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q</td>\n",
       "      <td>多么中二的剧情，就是一群小青年想玩文艺，来着全人类做铺垫的故事，三星完全是给作画的，剧情就是一坨屎</td>\n",
       "      <td>3</td>\n",
       "      <td>多么 中二 的 剧情   就是 一群 小青年 想 玩 文艺   来 着 全人类 做 铺垫 的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>38</td>\n",
       "      <td>https://movie.douban.com/subject/2567647/</td>\n",
       "      <td>福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q</td>\n",
       "      <td>疯了</td>\n",
       "      <td>5</td>\n",
       "      <td>疯 了</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>39</td>\n",
       "      <td>https://movie.douban.com/subject/2567647/</td>\n",
       "      <td>福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q</td>\n",
       "      <td>gay死它得了。碇真治对㶆薰那一脸的菊花湿润瘙痒啊。。。</td>\n",
       "      <td>3</td>\n",
       "      <td>gay 死 它 得 了   碇真治 对 㶆 薰 那 一脸 的 菊花 湿润 瘙痒 啊</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>40</td>\n",
       "      <td>https://movie.douban.com/subject/2567647/</td>\n",
       "      <td>福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q</td>\n",
       "      <td>等我明白了再决定是不是加一星= =</td>\n",
       "      <td>4</td>\n",
       "      <td>等 我 明白 了 再 决定 是不是 加一星</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>1</td>\n",
       "      <td>https://movie.douban.com/subject/2567647/</td>\n",
       "      <td>福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q</td>\n",
       "      <td>盼了两年，火急火燎全神贯注地看完后……吭爹啊！除了基佬薰必须成一坨血的结局外，这货跟老版TV...</td>\n",
       "      <td>5</td>\n",
       "      <td>盼 了 两年   火急火燎 全神贯注 地看 完后     吭爹 啊   除了 基佬 薰 必须...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>2</td>\n",
       "      <td>https://movie.douban.com/subject/2567647/</td>\n",
       "      <td>福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q</td>\n",
       "      <td>时隔那么多年，明日香又登女神榜，薰继续是大爱！可惜，我依旧讨厌圣母小白什么的！</td>\n",
       "      <td>4</td>\n",
       "      <td>时隔 那么 多年   明日香 又 登 女神 榜   薰 继续 是 大爱   可惜   我 依...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                       link                       name  \\\n",
       "560  33  https://movie.douban.com/subject/2567647/  福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q   \n",
       "561  34  https://movie.douban.com/subject/2567647/  福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q   \n",
       "562  35  https://movie.douban.com/subject/2567647/  福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q   \n",
       "563  36  https://movie.douban.com/subject/2567647/  福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q   \n",
       "564  37  https://movie.douban.com/subject/2567647/  福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q   \n",
       "565  38  https://movie.douban.com/subject/2567647/  福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q   \n",
       "566  39  https://movie.douban.com/subject/2567647/  福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q   \n",
       "567  40  https://movie.douban.com/subject/2567647/  福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q   \n",
       "569   1  https://movie.douban.com/subject/2567647/  福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q   \n",
       "570   2  https://movie.douban.com/subject/2567647/  福音战士新剧场版：Q ヱヴァンゲリヲン新劇場版：Q   \n",
       "\n",
       "                                               comment star  \\\n",
       "560                                  2013.04.25 掉渣天的画质    5   \n",
       "561  只有凌波，我一定要救出来！新加：上映当天就被剧透到死了，今天枪版观看完毕，说不上来的感觉，是...    5   \n",
       "562  距离发动失败的第三次冲击已经14年的时间，整个世界已经不一样，碇真嗣再一次被导向发动第四次冲...    4   \n",
       "563                             这到底神马情况，各种乱入..........    3   \n",
       "564  多么中二的剧情，就是一群小青年想玩文艺，来着全人类做铺垫的故事，三星完全是给作画的，剧情就是一坨屎    3   \n",
       "565                                                 疯了    5   \n",
       "566                       gay死它得了。碇真治对㶆薰那一脸的菊花湿润瘙痒啊。。。    3   \n",
       "567                                  等我明白了再决定是不是加一星= =    4   \n",
       "569  盼了两年，火急火燎全神贯注地看完后……吭爹啊！除了基佬薰必须成一坨血的结局外，这货跟老版TV...    5   \n",
       "570            时隔那么多年，明日香又登女神榜，薰继续是大爱！可惜，我依旧讨厌圣母小白什么的！    4   \n",
       "\n",
       "                                           content_cut  \n",
       "560                        2013   04   25   掉 渣 天 的 画质  \n",
       "561  只有 凌波   我 一定 要 救 出来   新加   上映 当天 就 被 剧透到 死 了  ...  \n",
       "562  距离 发动 失败 的 第三次 冲击 已经 14 年 的 时间   整个 世界 已经 不 一样...  \n",
       "563             这 到底 神马 情况   各种 乱入                      \n",
       "564  多么 中二 的 剧情   就是 一群 小青年 想 玩 文艺   来 着 全人类 做 铺垫 的...  \n",
       "565                                                疯 了  \n",
       "566    gay 死 它 得 了   碇真治 对 㶆 薰 那 一脸 的 菊花 湿润 瘙痒 啊        \n",
       "567                        等 我 明白 了 再 决定 是不是 加一星        \n",
       "569  盼 了 两年   火急火燎 全神贯注 地看 完后     吭爹 啊   除了 基佬 薰 必须...  \n",
       "570  时隔 那么 多年   明日香 又 登 女神 榜   薰 继续 是 大爱   可惜   我 依...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.iloc[560:570]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261494"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'燃   大 场面 真的 不输 国外 大片 不 尴尬   吴京 打戏 很 精彩   水下 搏斗 看着 也 很 有力   必须 安利 一下张 翰   这 角色 简直 就是 个 彩蛋 啊   承包 所有 笑点   为 他 量身定做 的 哈哈哈   彭于 晏 可演 不来   是 真的 好看'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_all.iloc[55].content_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#建立训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import os\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_sentences(sentences, embedding_size = 128, window = 5, min_count = 5, file_to_load = None, file_to_save = None):\n",
    "    if file_to_load is not None:\n",
    "        w2vModel = Word2Vec.load(file_to_load)\n",
    "    else:\n",
    "        w2vModel = Word2Vec(sentences, size = embedding_size, window = window, min_count = min_count, workers = multiprocessing.cpu_count())\n",
    "        if file_to_save is not None:\n",
    "            w2vModel.save(file_to_save)\n",
    "    all_vectors = []\n",
    "    embeddingDim = w2vModel.vector_size\n",
    "    embeddingUnknown = [0 for i in range(embeddingDim)]\n",
    "    for sentence in sentences:\n",
    "        this_vector = []\n",
    "        for word in sentence:\n",
    "            if word in w2vModel.wv.vocab:\n",
    "                this_vector.append(w2vModel[word])\n",
    "            else:\n",
    "                this_vector.append(embeddingUnknown)\n",
    "        all_vectors.append(this_vector)\n",
    "    return all_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_sentences(input_sentences, padding_token, padding_sentence_length = None):\n",
    "    sentences = [sentence.split(' ') for sentence in input_sentences]\n",
    "    max_sentence_length = padding_sentence_length if padding_sentence_length is not None else max([len(sentence) for sentence in sentences])\n",
    "    for sentence in sentences:\n",
    "        if len(sentence) > max_sentence_length:\n",
    "            sentence = sentence[:max_sentence_length]\n",
    "        else:\n",
    "            sentence.extend([padding_token] * (max_sentence_length - len(sentence)))\n",
    "    return (sentences, max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_content, max_document_length_content = padding_sentences(list(content_all.content_cut[:10000]), '<PADDING>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261494"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(content_all.content_cut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "x = np.array(embedding_sentences(sentences_content, embedding_size = 128, file_to_save = os.path.join(\"./\", 'trained_word2vec.model')),dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 195, 128)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(content_all.star[:10000].values,dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, 4, 1, 1, 2, 4, 4, 1, 1, 4, 1, 2, 2, 5, 2, 3, 3, 2, 4, 4,\n",
       "       4, 1, 4, 2, 2, 4, 1, 4, 3, 4, 5, 1, 3, 1, 4, 2, 2, 2, 1, 3, 5, 3,\n",
       "       4, 3, 3, 5, 2, 1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# one_hot=tf.one_hot(y,5)\n",
    " \n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     y_one = sess.run(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one_hot\n",
    "def reformat(labels):\n",
    "#   Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  labels = (np.arange(5) == labels[:,None]).astype(np.float32)\n",
    "  return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_one = reformat(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_one[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split#直接用交叉验证?\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y_one, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 195, 128)\n",
      "(3000, 195, 128)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gensim,one_hot转词向量，3中[2,3,4]词数量组合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建连接model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length=x.shape[1]\n",
    "num_classes=y_one.shape[1]\n",
    "embedding_size=128\n",
    "filter_sizes=[2,3,4]\n",
    "num_filters= 64\n",
    "l2_reg_lambda=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a(?, 195, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "input_x = tf.placeholder(tf.float32, [None,sequence_length, embedding_size], name = \"input_x\")\n",
    "input_y = tf.placeholder(tf.float32, [None, num_classes], name = \"input_y\")\n",
    "dropout_keep_prob = tf.placeholder(tf.float32, name = \"dropout_keep_prob\")\n",
    "\n",
    "# Keeping track of l2 regularization loss (optional)\n",
    "l2_loss = tf.constant(0.0)\n",
    "\n",
    "# Embedding layer\n",
    "    # self.embedded_chars = [None(batch_size), sequence_size, embedding_size]\n",
    "    # self.embedded_chars = [None(batch_size), sequence_size, embedding_size, 1(num_channels)]\n",
    "embedded_chars = input_x\n",
    "embedded_chars_expended = tf.expand_dims(embedded_chars, -1)\n",
    "print(\"a{}\".format(embedded_chars_expended.shape))#x_image = tf.reshape(x_, [-1,195,128,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "pooled_outputs = []\n",
    "for i, filter_size in enumerate(filter_sizes):\n",
    "\n",
    "    with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "   # Convolution layer\n",
    "        filter_shape = [filter_size, embedding_size, 1, num_filters]#h（几个单词）,w[128],1维（如果有多个维度，就多维度数据相加），128（想要128个特征，也就是filters层数）[2,128,1,128]\n",
    "        W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")#\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "        conv = tf.nn.conv2d(embedded_chars_expended,\n",
    "                    W,\n",
    "                    strides=[1,1,1,1],\n",
    "                    padding=\"VALID\",\n",
    "                   name=\"conv\")\n",
    "# Apply nonlinearity\n",
    "        h = tf.nn.relu(tf.nn.bias_add(conv, b), name = \"relu\")#b是一维\n",
    "# Maxpooling over the outputs\n",
    "        pooled = tf.nn.max_pool(\n",
    "                                h,\n",
    "                                ksize=[1, sequence_length - filter_size + 1, 1, 1],#\n",
    "                                strides=[1,1,1,1],\n",
    "                                padding=\"VALID\",\n",
    "                                name=\"pool\")\n",
    "        pooled_outputs.append(pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'conv-maxpool-2/pool:0' shape=(?, 1, 1, 64) dtype=float32>,\n",
       " <tf.Tensor 'conv-maxpool-3/pool:0' shape=(?, 1, 1, 64) dtype=float32>,\n",
       " <tf.Tensor 'conv-maxpool-4/pool:0' shape=(?, 1, 1, 64) dtype=float32>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-44-f91e7b656a05>:7: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Combine all the pooled features\n",
    "num_filters_total = num_filters * len(filter_sizes)\n",
    "h_pool = tf.concat(pooled_outputs,3)\n",
    "h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])#3个向量叠加起来\n",
    "# Add dropout\n",
    "with tf.name_scope(\"dropout\"):\n",
    "    h_drop = tf.nn.dropout(h_pool_flat, dropout_keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dropout/dropout/mul:0' shape=(?, 192) dtype=float32>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1) # 取随机值，符合均值为0，标准差stddev为0.1\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final (unnomalized) scores and predictions\n",
    "\n",
    "with tf.name_scope(\"output\"):\n",
    "#     tf.reset_default_graph()\n",
    "    W_f = weight_variable([64*3*1, 5])\n",
    "#     W_f = tf.Variable(\"W_f2\",shape = [64*3*1, 5],initializer = tf.contrib.layers.xavier_initializer())\n",
    "#     b_f = tf.Variable(tf.constant(0.1, shape=[num_classes], name = \"b\"))\n",
    "    b_f = bias_variable([num_classes])\n",
    "    l2_loss += tf.nn.l2_loss(W_f)\n",
    "    l2_loss += tf.nn.l2_loss(b_f)\n",
    "    scores = tf.nn.xw_plus_b(h_drop, W_f, b_f, name = \"scores\")\n",
    "    predictions = tf.argmax(scores, 1, name = \"predictions\")\n",
    "\n",
    "# Calculate Mean cross-entropy loss\n",
    "with tf.name_scope(\"loss\"):\n",
    "    losses = tf.nn.softmax_cross_entropy_with_logits_v2(logits = scores, labels = input_y)\n",
    "    loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "# Accuracy\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_predictions = tf.equal(predictions, tf.argmax(input_y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name = \"accuracy\")\n",
    "# correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1)) \n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为训练过程指定最小化误差用的损失函数，即目标类别和预测类别之间的交叉熵\n",
    "cross_entropy = -tf.reduce_sum(input_y*tf.log(loss))\n",
    "\n",
    "# 使用反向传播，利用优化器使损失函数最小化\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)#GradientDescentOptimizer(0.5) #梯度下降,0.5学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.23\n",
      "step 100, training accuracy 0.22\n",
      "step 200, training accuracy 0.29\n",
      "step 300, training accuracy 0.21\n",
      "step 400, training accuracy 0.29\n",
      "step 500, training accuracy 0.26\n",
      "step 600, training accuracy 0.26\n",
      "step 700, training accuracy 0.31\n",
      "step 800, training accuracy 0.27\n",
      "step 900, training accuracy 0.27\n",
      "test accuracy 0.266333\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver() # 定义saver\n",
    "batch_size = 100\n",
    "# *************** 开始训练模型 *************** #\n",
    "# one_hot=tf.one_hot(y,5)\n",
    " \n",
    "with tf.Session().as_default() as sess2:\n",
    "    sess2.run(tf.global_variables_initializer())\n",
    "#     y_one = sess.run(one_hot)\n",
    "    for i in range(1000):\n",
    "        offset = (i * batch_size) % (X_train.shape[0] - batch_size)#这句是防止迭代次数过多超出数据集范围，就通过取余数改变取batch的偏置\n",
    "        batch_data = X_train[offset:(offset + batch_size), :, :]\n",
    "        batch_labels = y_train[offset:(offset + batch_size), :]\n",
    "#         print(batch_data.shape)\n",
    "#         print(batch_labels.shape)\n",
    "        if i%100 == 0:\n",
    "        # 评估模型准确度，此阶段不使用Dropout\n",
    "            train_accuracy = accuracy.eval(feed_dict={input_x:batch_data, input_y:batch_labels, dropout_keep_prob: 1.0})#概率是1，也就是说 在测试时保留所有连接\n",
    "            \n",
    "            print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "\n",
    "      # 训练模型，此阶段使用50%的Dropout\n",
    "        train_step.run(feed_dict={input_x: batch_data, input_y: batch_labels, dropout_keep_prob: 0.5}) \n",
    "\n",
    "    saver.save(sess2, './save1/model.ckpt') #模型储存位置\n",
    "\n",
    "    print(\"test accuracy %g\"%accuracy.eval(feed_dict={input_x: X_test,input_y: y_test, dropout_keep_prob: 1.0}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网上代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    '''\n",
    "    Generate a batch iterator for a dataset\n",
    "    '''\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((data_size - 1) / batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        if shuffle:\n",
    "        # Shuffle the data at each epoch\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "    for batch_num in range(num_batches_per_epoch):\n",
    "        start_idx = batch_num * batch_size\n",
    "        end_idx = min((batch_num + 1) * batch_size, data_size)\n",
    "        yield shuffled_data[start_idx : end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    '''\n",
    "    A CNN for text classification\n",
    "    Uses and embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    '''\n",
    "    def __init__(\n",
    "        self, sequence_length, num_classes,\n",
    "        embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output, dropout\n",
    "        self.input_x = tf.placeholder(tf.float32, [None, sequence_length, embedding_size], name = \"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name = \"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name = \"dropout_keep_prob\")\n",
    "        \n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "            # self.embedded_chars = [None(batch_size), sequence_size, embedding_size]\n",
    "            # self.embedded_chars = [None(batch_size), sequence_size, embedding_size, 1(num_channels)]\n",
    "        self.embedded_chars = self.input_x\n",
    "        self.embedded_chars_expended = tf.expand_dims(self.embedded_chars, -1)\n",
    "        print(\"a{}\".format(self.embedded_chars_expended.shape))\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            \n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]#h（几个单词）,w,1维，128（想要128个特征）\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(self.embedded_chars_expended,\n",
    "                                    W,\n",
    "                                    strides=[1,1,1,1],\n",
    "                                    padding=\"VALID\",\n",
    "                            name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name = \"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                h,\n",
    "                ksize=[1, sequence_length - filter_size + 1, 1, 1],#\n",
    "                strides=[1,1,1,1],\n",
    "                padding=\"VALID\",\n",
    "                name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs,3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "        \n",
    "        # Final (unnomalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                                \"W\",\n",
    "                                    shape = [num_filters_total, num_classes],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes], name = \"b\"))\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name = \"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name = \"predictions\")\n",
    "\n",
    "        # Calculate Mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits_v2(logits = self.scores, labels = self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "        \n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name = \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_train, y_train,  x_dev, y_dev):\n",
    "\n",
    "    # Training\n",
    "\n",
    "    # ==================================================\n",
    "\n",
    "\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "        allow_soft_placement = True,\n",
    "        log_device_placement = False)\n",
    "        sess = tf.Session(config = session_conf)\n",
    "        with sess.as_default():\n",
    "            cnn = TextCNN(\n",
    "            sequence_length = x_train.shape[1],\n",
    "            num_classes = y_train.shape[1],\n",
    "            embedding_size = 128,\n",
    "            filter_sizes = [2,3,4],\n",
    "            num_filters = 64,\n",
    "            l2_reg_lambda = 0.0)\n",
    "\n",
    "        # Define Training procedure\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn.loss)#计算梯度\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)##处理梯度\n",
    "\n",
    "            # Keep track of gradient values and sparsity (optional)\n",
    "            grad_summaries = []\n",
    "            for g, v in grads_and_vars:\n",
    "                if g is not None:\n",
    "                    grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                    sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                    grad_summaries.append(grad_hist_summary)\n",
    "                    grad_summaries.append(sparsity_summary)\n",
    "            grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 0.5\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train, y_train)), 64, 200)\n",
    "\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % 100 == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % 100 == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a(?, 195, 128, 1)\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/hist is illegal; using conv-maxpool-2/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/sparsity is illegal; using conv-maxpool-2/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/hist is illegal; using conv-maxpool-2/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/sparsity is illegal; using conv-maxpool-2/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/Variable:0/grad/hist is illegal; using output/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/Variable:0/grad/sparsity is illegal; using output/Variable_0/grad/sparsity instead.\n",
      "Writing to E:\\新桌面\\Mlearning\\AI上课\\上课10\\作业10\\runs\\1569108255\n",
      "\n",
      "2019-09-22T08:01:19.491446: step 1, loss 8.39092, acc 0.140625\n",
      "2019-09-22T08:01:19.668989: step 2, loss 6.35761, acc 0.171875\n",
      "2019-09-22T08:01:19.829213: step 3, loss 4.1924, acc 0.203125\n",
      "2019-09-22T08:01:19.994145: step 4, loss 2.8989, acc 0.296875\n",
      "2019-09-22T08:01:20.145530: step 5, loss 2.47186, acc 0.25\n",
      "2019-09-22T08:01:20.312298: step 6, loss 2.29249, acc 0.234375\n",
      "2019-09-22T08:01:20.466671: step 7, loss 2.76264, acc 0.28125\n",
      "2019-09-22T08:01:20.632193: step 8, loss 3.51714, acc 0.234375\n",
      "2019-09-22T08:01:20.813471: step 9, loss 2.18004, acc 0.25\n",
      "2019-09-22T08:01:20.996994: step 10, loss 3.8954, acc 0.25\n",
      "2019-09-22T08:01:21.173967: step 11, loss 2.1026, acc 0.3125\n",
      "2019-09-22T08:01:21.342046: step 12, loss 2.02453, acc 0.21875\n",
      "2019-09-22T08:01:21.516344: step 13, loss 2.10496, acc 0.265625\n",
      "2019-09-22T08:01:21.693470: step 14, loss 2.90786, acc 0.203125\n",
      "2019-09-22T08:01:21.857437: step 15, loss 1.85601, acc 0.234375\n",
      "2019-09-22T08:01:22.009961: step 16, loss 2.48692, acc 0.203125\n",
      "2019-09-22T08:01:22.160335: step 17, loss 2.26139, acc 0.1875\n",
      "2019-09-22T08:01:22.316207: step 18, loss 2.26647, acc 0.296875\n",
      "2019-09-22T08:01:22.466452: step 19, loss 1.8927, acc 0.296875\n",
      "2019-09-22T08:01:22.624137: step 20, loss 2.26168, acc 0.21875\n",
      "2019-09-22T08:01:22.770505: step 21, loss 2.63741, acc 0.28125\n",
      "2019-09-22T08:01:22.927076: step 22, loss 2.25685, acc 0.234375\n",
      "2019-09-22T08:01:23.081879: step 23, loss 2.61735, acc 0.203125\n",
      "2019-09-22T08:01:23.247605: step 24, loss 2.42999, acc 0.234375\n",
      "2019-09-22T08:01:23.402948: step 25, loss 2.21245, acc 0.34375\n",
      "2019-09-22T08:01:23.567635: step 26, loss 2.4681, acc 0.234375\n",
      "2019-09-22T08:01:23.720445: step 27, loss 1.94363, acc 0.25\n",
      "2019-09-22T08:01:23.871199: step 28, loss 2.71982, acc 0.171875\n",
      "2019-09-22T08:01:24.030901: step 29, loss 2.18243, acc 0.1875\n",
      "2019-09-22T08:01:24.189889: step 30, loss 2.41623, acc 0.140625\n",
      "2019-09-22T08:01:24.344601: step 31, loss 2.45995, acc 0.171875\n",
      "2019-09-22T08:01:24.511467: step 32, loss 2.11994, acc 0.125\n",
      "2019-09-22T08:01:24.676521: step 33, loss 2.25164, acc 0.234375\n",
      "2019-09-22T08:01:24.836926: step 34, loss 2.90167, acc 0.125\n",
      "2019-09-22T08:01:25.001099: step 35, loss 2.78034, acc 0.1875\n",
      "2019-09-22T08:01:25.165427: step 36, loss 2.7553, acc 0.234375\n",
      "2019-09-22T08:01:25.327539: step 37, loss 2.44401, acc 0.25\n",
      "2019-09-22T08:01:25.480678: step 38, loss 2.2215, acc 0.1875\n",
      "2019-09-22T08:01:25.627001: step 39, loss 2.76577, acc 0.203125\n",
      "2019-09-22T08:01:25.783446: step 40, loss 2.39997, acc 0.234375\n",
      "2019-09-22T08:01:25.938694: step 41, loss 2.61638, acc 0.21875\n",
      "2019-09-22T08:01:26.094334: step 42, loss 2.53652, acc 0.25\n",
      "2019-09-22T08:01:26.251655: step 43, loss 2.12266, acc 0.21875\n",
      "2019-09-22T08:01:26.417763: step 44, loss 2.07622, acc 0.296875\n",
      "2019-09-22T08:01:26.571195: step 45, loss 2.55114, acc 0.234375\n",
      "2019-09-22T08:01:26.734923: step 46, loss 3.30356, acc 0.15625\n",
      "2019-09-22T08:01:26.914615: step 47, loss 2.55446, acc 0.265625\n",
      "2019-09-22T08:01:27.113763: step 48, loss 2.35186, acc 0.265625\n",
      "2019-09-22T08:01:27.286086: step 49, loss 2.48475, acc 0.125\n",
      "2019-09-22T08:01:27.457340: step 50, loss 2.38091, acc 0.25\n",
      "2019-09-22T08:01:27.641098: step 51, loss 2.36801, acc 0.25\n",
      "2019-09-22T08:01:27.815148: step 52, loss 2.73403, acc 0.171875\n",
      "2019-09-22T08:01:27.966269: step 53, loss 2.30125, acc 0.3125\n",
      "2019-09-22T08:01:28.143329: step 54, loss 2.72166, acc 0.203125\n",
      "2019-09-22T08:01:28.302668: step 55, loss 2.53266, acc 0.21875\n",
      "2019-09-22T08:01:28.468409: step 56, loss 3.67966, acc 0.171875\n",
      "2019-09-22T08:01:28.639348: step 57, loss 1.8462, acc 0.3125\n",
      "2019-09-22T08:01:28.799095: step 58, loss 2.90016, acc 0.21875\n",
      "2019-09-22T08:01:28.966002: step 59, loss 2.15398, acc 0.28125\n",
      "2019-09-22T08:01:29.123261: step 60, loss 2.15031, acc 0.28125\n",
      "2019-09-22T08:01:29.292539: step 61, loss 3.18417, acc 0.171875\n",
      "2019-09-22T08:01:29.452055: step 62, loss 3.10592, acc 0.21875\n",
      "2019-09-22T08:01:29.615505: step 63, loss 3.26432, acc 0.125\n",
      "2019-09-22T08:01:29.772535: step 64, loss 3.39781, acc 0.15625\n",
      "2019-09-22T08:01:29.939725: step 65, loss 2.7861, acc 0.125\n",
      "2019-09-22T08:01:30.094877: step 66, loss 2.22894, acc 0.265625\n",
      "2019-09-22T08:01:30.263301: step 67, loss 2.73253, acc 0.296875\n",
      "2019-09-22T08:01:30.424497: step 68, loss 2.15587, acc 0.234375\n",
      "2019-09-22T08:01:30.593234: step 69, loss 2.60034, acc 0.28125\n",
      "2019-09-22T08:01:30.754646: step 70, loss 3.0749, acc 0.203125\n",
      "2019-09-22T08:01:30.925233: step 71, loss 2.87044, acc 0.1875\n",
      "2019-09-22T08:01:31.084603: step 72, loss 2.21775, acc 0.25\n",
      "2019-09-22T08:01:31.247736: step 73, loss 2.96729, acc 0.140625\n",
      "2019-09-22T08:01:31.407111: step 74, loss 3.57516, acc 0.265625\n",
      "2019-09-22T08:01:31.580978: step 75, loss 3.41447, acc 0.140625\n",
      "2019-09-22T08:01:31.737443: step 76, loss 2.95569, acc 0.21875\n",
      "2019-09-22T08:01:31.902465: step 77, loss 3.57577, acc 0.3125\n",
      "2019-09-22T08:01:32.060426: step 78, loss 2.19467, acc 0.21875\n",
      "2019-09-22T08:01:32.229646: step 79, loss 3.61598, acc 0.234375\n",
      "2019-09-22T08:01:32.386661: step 80, loss 3.07305, acc 0.171875\n",
      "2019-09-22T08:01:32.558468: step 81, loss 2.79937, acc 0.171875\n",
      "2019-09-22T08:01:32.720147: step 82, loss 3.44499, acc 0.234375\n",
      "2019-09-22T08:01:32.892657: step 83, loss 3.68548, acc 0.203125\n",
      "2019-09-22T08:01:33.070035: step 84, loss 2.91654, acc 0.140625\n",
      "2019-09-22T08:01:33.268579: step 85, loss 3.71814, acc 0.15625\n",
      "2019-09-22T08:01:33.459720: step 86, loss 1.99396, acc 0.265625\n",
      "2019-09-22T08:01:33.636830: step 87, loss 2.94642, acc 0.359375\n",
      "2019-09-22T08:01:33.816472: step 88, loss 4.14691, acc 0.21875\n",
      "2019-09-22T08:01:33.965468: step 89, loss 2.77106, acc 0.1875\n",
      "2019-09-22T08:01:34.124629: step 90, loss 2.89781, acc 0.28125\n",
      "2019-09-22T08:01:34.279955: step 91, loss 2.79964, acc 0.203125\n",
      "2019-09-22T08:01:34.440503: step 92, loss 2.77921, acc 0.296875\n",
      "2019-09-22T08:01:34.603493: step 93, loss 4.48054, acc 0.125\n",
      "2019-09-22T08:01:34.760882: step 94, loss 3.52844, acc 0.203125\n",
      "2019-09-22T08:01:34.925350: step 95, loss 3.2863, acc 0.234375\n",
      "2019-09-22T08:01:35.085830: step 96, loss 4.17498, acc 0.203125\n",
      "2019-09-22T08:01:35.239245: step 97, loss 3.52079, acc 0.203125\n",
      "2019-09-22T08:01:35.402623: step 98, loss 2.42544, acc 0.15625\n",
      "2019-09-22T08:01:35.555397: step 99, loss 3.19974, acc 0.3125\n",
      "2019-09-22T08:01:35.717949: step 100, loss 3.73356, acc 0.28125\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-22T08:01:38.263012: step 100, loss 1.90455, acc 0.317\n",
      "\n",
      "Saved model checkpoint to E:\\新桌面\\Mlearning\\AI上课\\上课10\\作业10\\runs\\1569108255\\checkpoints\\model-100\n",
      "\n",
      "2019-09-22T08:01:39.556647: step 101, loss 3.06685, acc 0.234375\n",
      "2019-09-22T08:01:39.858532: step 102, loss 2.84295, acc 0.25\n",
      "2019-09-22T08:01:40.026434: step 103, loss 4.68455, acc 0.234375\n",
      "2019-09-22T08:01:40.190147: step 104, loss 4.09302, acc 0.265625\n",
      "2019-09-22T08:01:40.349505: step 105, loss 4.29363, acc 0.265625\n",
      "2019-09-22T08:01:40.501453: step 106, loss 4.36346, acc 0.28125\n",
      "2019-09-22T08:01:40.653675: step 107, loss 3.79608, acc 0.1875\n",
      "2019-09-22T08:01:40.812599: step 108, loss 5.16009, acc 0.171875\n",
      "2019-09-22T08:01:40.968929: step 109, loss 3.4882, acc 0.203125\n",
      "2019-09-22T08:01:41.049143: step 110, loss 4.31981, acc 0.25\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络，TFIDF转词向量，全连接，效果不好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_pattern这个参数使用正则表达式来分词，其默认参数为r\"(?u)\\b\\w\\w+\\b\"，其中的两个\\w决定了其匹配长度至少为2的单词\n",
    "# 长度为1的单词在英文中一般是无足轻重的，但在中文里，就可能有一些很重要的单字词\n",
    "# 当设置为浮点数时，过滤出现在超过max_df/低于min_df比例的句子中的词语；正整数时,则是超过max_df句句子。\n",
    "vectorized = TfidfVectorizer(max_features=1000, token_pattern=r\"(?u)\\b\\w\\w+\\b\", max_df = 1.0,\n",
    "                                 stop_words = [\"是\", \"的\",\"了\"],vocabulary = None )#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorized = TfidfVectorizer(max_features=10000)#(max_features=10000)#TfidfVectorizer 完成向量化与 TF-IDF 预处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vectorized.fit_transform(content_all.content_cut).toarray()#这里转换稀疏矩阵，占用内存太大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261494, 1000)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.82602493,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.56363358, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[49999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.array(content_all.star.values,dtype=int)#转列表,定义类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[49999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183045\n",
      "183045\n",
      "78449\n",
      "78449\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split#直接用交叉验证?\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(len(X_test))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78449, 1000)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (183045, 1000) (183045, 5)\n",
      "Test set (78449, 1000) (78449, 5)\n"
     ]
    }
   ],
   "source": [
    "# image_size = 28\n",
    "num_labels = 5\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, 1000)).astype(np.float32)\n",
    "#   Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(X_train, y_train)\n",
    "# valid_dataset, valid_labels = reformat(X_test, y_test)\n",
    "test_dataset, test_labels = reformat(X_test, y_test)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "# print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#下一步我们先定义一个用来检测预测精度的方法：\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])#注意这里的argmax方法返回的是数组的索引值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "batch_size = 32\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "#   tf.cast(X_test,tf.float32)\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, 1000))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, 5))\n",
    "#   tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "#   Variables.\n",
    "  weights = tf.Variable(tf.truncated_normal([1000, 5]))\n",
    "  biases = tf.Variable(tf.zeros([5]))\n",
    "  #model\n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  relu_layer= tf.nn.relu(logits)#relu的作用，激活函数\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=relu_layer))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)#SGD\n",
    "\n",
    "#     optimizer = tf.train.AdamOptimizer(0.1, epsilon=1e-08).minimize(loss)#默认的learning_rate=0.001，ADAM\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "#   valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2层，为什么用2层效果反而更差？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "embedding_size = 1000\n",
    "num_nodes= 1024\n",
    "batch_size = 150\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, embedding_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "#     tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.    \n",
    "    weights_1 = tf.Variable(tf.truncated_normal([embedding_size, num_nodes]))\n",
    "    biases_1 = tf.Variable(tf.zeros([num_nodes]))\n",
    "    weights_2 = tf.Variable(tf.truncated_normal([num_nodes, num_labels]))\n",
    "    biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training computation.\n",
    "    logits_1 = tf.matmul(tf_train_dataset, weights_1) + biases_1\n",
    "    relu_layer= tf.nn.relu(logits_1)\n",
    "    \n",
    "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "#     relu_layer_2= tf.nn.relu(logits_2)#不rule,loss巨大？模型不对？\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits_2))\n",
    "\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training\n",
    "    train_prediction = tf.nn.softmax(logits_2)#和relu_layer_2一样？\n",
    "    \n",
    "#     # Predictions for validation \n",
    "#     logits_1 = tf.matmul(tf_valid_dataset, weights_1) + biases_1\n",
    "#     relu_layer= tf.nn.relu(logits_1)\n",
    "#     logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "    \n",
    "#     valid_prediction = tf.nn.softmax(logits_2)\n",
    "    \n",
    "#     # Predictions for test\n",
    "#     logits_1 = tf.matmul(tf_test_dataset, weights_1) + biases_1\n",
    "#     relu_layer= tf.nn.relu(logits_1)\n",
    "#     logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "    \n",
    "#     test_prediction = tf.nn.softmax(logits_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 11.345606\n",
      "Minibatch accuracy: 26.0%\n",
      "Minibatch loss at step 500: 98555854848.000000\n",
      "Minibatch accuracy: 24.7%\n",
      "Minibatch loss at step 1000: 378216835490573713408.000000\n",
      "Minibatch accuracy: 16.0%\n",
      "Minibatch loss at step 1500: 6821544067122667698026629300224.000000\n",
      "Minibatch accuracy: 36.0%\n",
      "Minibatch loss at step 2000: nan\n",
      "Minibatch accuracy: 29.3%\n",
      "Minibatch loss at step 2500: nan\n",
      "Minibatch accuracy: 24.0%\n",
      "Minibatch loss at step 3000: nan\n",
      "Minibatch accuracy: 21.3%\n",
      "Minibatch loss at step 3500: nan\n",
      "Minibatch accuracy: 22.0%\n",
      "Minibatch loss at step 4000: nan\n",
      "Minibatch accuracy: 25.3%\n",
      "Minibatch loss at step 4500: nan\n",
      "Minibatch accuracy: 21.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5000#随机 抽取steps次，（一直训练）\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "#   init = tf.global_variables_initializer()\n",
    "#   session.run(init)\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_dataset.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "#       print(\"Minibatch loss at step {}: {}\".format(step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "#       print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "#   print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
